---
title: "AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning"
date: 2024-02-08
comments: true
categories: [Computer Science, Computer Vision and Pattern Recognition]
---

# AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning
- <https://arxiv.org/abs/2307.04725>
- <https://arxiv.org/pdf/2307.04725>

---
## AnimateDiff: 既存の画像生成モデルをアニメーション生成に拡張する手法

## 1. 研究の背景と目的

近年、テキストから画像を生成する技術（Text-to-Image, T2I）が大きく進歩し、Stable Diffusionなどのモデルが一般に利用可能になりました。さらに、DreamBoothやLoRAなどの手法により、少量のデータでこれらのモデルをカスタマイズ（パーソナライズ）することが可能になりました。

しかし、これらのパーソナライズされたT2Iモデルは静止画の生成に特化しており、アニメーションの生成には適していません。本研究では、既存のパーソナライズされたT2Iモデルを、追加の学習なしでアニメーション生成に拡張する手法「AnimateDiff」を提案しています。

## 2. AnimateDiffの概要

AnimateDiffは、以下の3つの主要なコンポーネントから構成されています：

1. ドメインアダプター
2. モーションモジュール
3. MotionLoRA

これらのコンポーネントを組み合わせることで、既存のT2Iモデルをアニメーション生成に拡張します。

![AnimateDiff Overview](https://github.com/user-attachments/assets/f612c00b-df9f-48a2-9629-fb6f09382c7b)


## 3. ドメインアダプター

ドメインアダプターの目的は、高品質な画像データセットで学習されたベースのT2Iモデルと、比較的低品質な動画データセットとの間の視覚的な分布の差を吸収することです。

- LoRA（Low-Rank Adaptation）を使用して実装
- T2Iモデルの自己注意層とクロス注意層に挿入
- 動画データセットからランダムにサンプリングしたフレームで学習

ドメインアダプターを使用することで、モーションモジュールが視覚的な品質の差ではなく、純粋に動きの学習に集中できるようになります。

## 4. モーションモジュール

モーションモジュールは、動画データから動きの事前知識を学習し、それをT2Iモデルに統合するための核心的なコンポーネントです。

- 時間軸に沿った「時間的Transformer」アーキテクチャを採用
- 2次元の拡散モデルを3次元の動画データに対応するよう拡張
- 事前学習された画像層は各フレームを独立して処理
- モーションモジュールは時間軸に沿って情報を交換

モーションモジュールにより、T2Iモデルは個々のフレームを独立して生成するのではなく、時間経過に伴う視覚コンテンツの変化を捉えることができるようになります。

## 5. MotionLoRA

MotionLoRAは、事前学習されたモーションモジュールを新しい動きのパターン（ズームイン、パンニングなど）に効率的に適応させるための軽量な微調整技術です。

- モーションモジュールの自己注意層にLoRA層を追加
- 少数の参照動画（20〜50程度）で学習可能
- 約2,000回の学習イテレーション（1〜2時間程度）で新しい動きを学習
- 学習済みモデルは約30MBと軽量

MotionLoRAにより、ユーザーは特定の動きのエフェクトに対してモーションモジュールを効率的に微調整できます。

## 6. AnimateDiffの学習と推論

### 学習プロセス

1. ドメインアダプターの学習
2. モーションモジュールの学習
3. （オプション）MotionLoRAの学習

各段階で、対象となるコンポーネント以外のパラメータは固定されます。

### 推論プロセス

1. パーソナライズされたT2Iモデルを3次元に拡張
2. モーションモジュールを挿入
3. （オプション）MotionLoRAを適用
4. 逆拡散プロセスを実行してアニメーションフレームを生成

## 7. 実験結果

AnimateDiffの性能を評価するために、さまざまなドメイン（2Dカートゥーンからリアルな写真まで）のパーソナライズされたT2Iモデルを使用して実験を行いました。

![Qualitative Results](https://github.com/user-attachments/assets/dd45c03b-8ab8-4ffb-b10a-ffb90a2825d4)

実験結果は以下の点を示しています：

- AnimateDiffは、さまざまなドメインのT2Iモデルに対して滑らかで視覚的に魅力的なアニメーションを生成できる
- MotionLoRAを使用することで、特定のカメラモーションを制御できる
- 既存の内容制御アプローチ（ControlNetなど）と組み合わせることが可能

## 8. 他手法との比較

AnimateDiffを以下の手法と比較しています：

1. Text2Video-Zero
2. Tune-a-Video
3. Gen-2（商用ツール）
4. Pika Labs（商用ツール）

ユーザー調査とCLIPメトリクスを用いた定量的な比較では、AnimateDiffが特にモーションの滑らかさにおいて優れた性能を示しました。

## 9. 倫理的配慮と再現性

研究チームは、生成AIの誤用に対する懸念を表明し、適切な使用を促しています。また、研究の再現性を確保するため、実装の詳細とコード、事前学習済みの重みを公開しています。

## 10. 結論と今後の展望

AnimateDiffは、既存のパーソナライズされたT2Iモデルをアニメーション生成に拡張する実用的な手法を提供しています。この技術は、映画やアニメーション産業など、さまざまな応用分野での利用が期待されます。

今後の研究課題としては、より長時間のアニメーション生成や、より複雑な動きのパターンへの対応などが考えられます。
